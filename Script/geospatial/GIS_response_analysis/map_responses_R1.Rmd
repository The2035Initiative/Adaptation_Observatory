# Map

## Load & Wrangle Survey Demographic Data
```{r message=FALSE, warning=FALSE}
library(sf)
library(here)
library(tmap)
library(terra)
library(stars)
library(leaflet)
#library(maptiles)
#library(devtools)
#library(ggspatial)
library(tmaptools)
#library(patchwork)
#library(sfheaders)
library(tidyverse)
#library(osmextract)
#library(tidygeocoder)
library(RColorBrewer)
```


```{r message=FALSE, warning=FALSE}
# load BGD data 
bgd <- read_csv("../results/clean_data/adaptation_observatory_bgd_cleaned_survey_questions_R1.csv",
                show_col_types = FALSE)  %>% 
  filter(complete.cases(location_longitude, location_latitude)) 
  


# load IND data 
ind <- read_csv("../results/clean_data/adaptation_observatory_ind_cleaned_survey_questions_R1.csv",
                show_col_types = FALSE) %>% 
  filter(complete.cases(location_longitude, location_latitude)) 


# ---- duplicates ----
# group duplicated by phone & ip
bgd_dup <- read_csv("../results/clean_data/adaptation_observatory_bgd_cleaned_duplicate_survey_questions_R1.csv",
                    show_col_types = FALSE) %>% 
  group_by(comp_phone1_try1, ip_address) %>% 
  filter(complete.cases(location_longitude, location_latitude)) 

# group duplicated by phone & ip
#ind_dup <- read_csv("../results/clean_data/adaptation_observatory_ind_cleaned_duplicate_survey_questions_R1.csv",
  #                  show_col_types = FALSE) %>% 
 # group_by(comp_phone1_try1, ip_address)%>% 
#  filter(complete.cases(location_longitude, location_latitude)) 


# ---- sf objects qualtrics location ----
# create sf objects
bgd_sf <- bgd %>% 
  st_as_sf(coords = c("location_longitude", "location_latitude"), crs = "EPSG:4326") %>% 
  st_make_valid()


ind_sf <- ind %>% 
  st_as_sf(coords = c("location_longitude", "location_latitude"), crs = "EPSG:4326") %>% 
  st_make_valid() %>% 
  filter(!is.na(geometry))

# ---  sf objects browser location ----
bgd_sf_browser <- bgd %>% 
  filter(!is.na(geo_lng) & !is.na(geo_lat)) %>%
  st_as_sf(coords = c("geo_lng", "geo_lat"), crs = "EPSG:4326") %>% 
  st_make_valid()


ind_sf_browser <- ind %>% 
  filter(!is.na(geo_lng) & !is.na(geo_lat)) %>%
  st_as_sf(coords = c("geo_lng", "geo_lat"), crs = "EPSG:4326") %>% 
  st_make_valid()

# ----sf objects qualtrics location duplicate ----
# for duplicates
bgd_dup_sf <- bgd_dup %>% 
  st_as_sf(coords = c("location_longitude", "location_latitude"), crs = "EPSG:4326") %>% 
  st_make_valid() %>% 
  filter(!is.na(geometry))

# for duplicates
ind_dup_sf <- ind_dup %>% 
  st_as_sf(coords = c("location_longitude", "location_latitude"), crs = "EPSG:4326") %>% 
  st_make_valid() %>% 
  filter(!is.na(geometry))
```

### Let's remove people outside our AOI
We'll do this using a st_join( .pred = st_intersects) using the shapefiles we used to generate the geo-points

```{r}
# read in the shape files
bgd_shp <- read_stars(here("../data/country_datas/bgd_pd_2020_1km.tif")) %>% 
  st_as_sf(crs = "EPSG:4326") %>% 
  st_make_valid()


ind_shp <- read_stars(here("../data/country_datas/ind_pd_2020_1km.tif")) %>% 
  st_as_sf(crs = "EPSG:4326") %>% 
  st_make_valid()
```

# Join

**Note to self, need to update clean_data with this**
```{r}
# Assuming you have a geometry column named 'geometry' in your data frames bgd and ind
# Extract the bounding box from the geometry column
bgd_bbox <- st_bbox(st_union(bgd_shp$geometry))

# Create bounding box polygons using sf
bgd_polygon <- st_as_sfc(st_bbox(bgd_bbox), crs = 4326) %>% 
  st_as_sf()

# Filter points within the AOI using st_intersection
# reduced down to 2238 from 2247
bgd_aoi <- st_intersection(bgd_sf, bgd_polygon)

# save as csv
st_write(bgd_aoi, 
         here("response_analysis/potential_panelists/bgd_ideal_survey_panelists.csv"),
         layer_options = "GEOMETRY=AS_XY")

# this takes a fat minute
ind_bbox <- st_bbox(st_union(ind_shp$geometry))

# Create bounding box polygons using sf
ind_polygon <- st_as_sfc(st_bbox(ind_bbox), crs = 4326) %>% 
  st_as_sf()

# Filter points within the AOI using st_intersection
# reduced down to 1213 from 1214
ind_aoi <- st_intersection(ind_sf, ind_polygon)

# save as csv
st_write(ind_aoi, 
         here("response_analysis/potential_panelists/ind_ideal_survey_panelists.csv"),
         layer_options = "GEOMETRY=AS_XY")
```


## Geospatial Analysis
For anoynmity purposes, it's best these plots don't render with the IP Addresses
```{r}
tmap_mode('view')

ind_response_map <- ind_aoi %>% 
  dplyr::select(-q_url) %>% 
  tm_shape()  +
  tm_basemap(leaflet::providers$Esri.WorldImagery) +
  tm_bubbles(alpha = 0.2,
             fill = 'ip_address',
             col = 'skyblue',) +
  tm_scalebar(position = c('left', 'bottom')) +
  tm_title("Adaptation Observatory Responses, Bangladesh (2024-4-11)")


tmap_save(ind_response_map,
        filename = here("response_analysis/response_visuals/IND_response_visuals/ind_aoi_map.png"))

ind_response_map
```


```{r}
tmap_mode('view')

bgd_response_map <- bgd_aoi %>% 
  dplyr::select(-q_url) %>% 
  tm_shape() +
  tm_basemap(leaflet::providers$Esri.WorldImagery) +
  tm_bubbles(alpha = 0.2,
             fill = 'ip_address',
             col = 'skyblue',) +
  tm_scalebar(position = c('left', 'bottom')) +
  tm_title("Adaptation Observatory Responses, Bangladesh (2024-4-11)")


tmap_save(bgd_response_map,
        filename = here("response_analysis/response_visuals/BGD_response_visuals/bgd_aoi_map.png"))

bgd_response_map
```

### What's the most number of surveys a single ip address has submitted?
```{r}
# let's check our new column
max(bgd_demos$number_of_surverys_submitted)

max(ind_demos$number_of_surverys_submitted)
```


### Simple Plot with Facebook Targeting
```{r}
# Load in Border Shapefiles
bgd_borders <- st_read(here("geospatial/country_shapefiles/geoBoundaries-BGD-ADM0-all/geoBoundaries-BGD-ADM0_simplified.shp"))

ind_borders <- st_read(here("geospatial/country_shapefiles/geoBoundaries-IND-ADM0-all/geoBoundaries-IND-ADM0_simplified.shp"))

# Import Facebook Point Targeting Data
bgd_fb <- read.csv(here("geospatial/surveying_points/bgd_survey_strat_points_1km_95th.csv"))

ind_fb <- read.csv(here("geospatial/surveying_points/ind_survey_strat_points_1km_95th.csv"))

bgd_fb_sf <- st_as_sf(bgd_fb, coords = c("X", "Y"), crs = 4326)
bgd_fb_sf_buffered <- st_buffer(bgd_fb_sf, dist = units::set_units(1000, "m"))

ind_fb_sf <- st_as_sf(ind_fb, coords = c("X", "Y"), crs = 4326)
ind_fb_sf_buffered <- st_buffer(ind_fb_sf, dist = units::set_units(1000, "m"))


# Drop respondent locations outside of country boundaries
bgd_sf_intersect <- st_intersection(bgd_sf, bgd_borders)
bgd_sf_browser_intersect <- st_intersection(bgd_sf_browser, bgd_borders)

ind_sf_intersect <- st_intersection(ind_sf, ind_borders)
ind_sf_browser_intersect <- st_intersection(ind_sf_browser, ind_borders)

# ---- bgd plot ----
bgd_plt <- ggplot() +
  geom_sf(data = bgd_sf_browser_intersect, aes(fill = "Respondents Browser Location", shape = "Respondents Browser Location"), color = "blue", size = 0.25, alpha = 0.5) + 
  geom_sf(data = bgd_sf_intersect, aes(fill = "Respondents Qualtrics Location", shape = "Respondents Qualtrics Location"), color = "skyblue", size = 0.25, alpha = 0.2) + 
  geom_sf(data = bgd_fb_sf_buffered, aes(fill = "Targeted Points", shape = "Targeted Points"), alpha = 1, color = NA) +
  geom_sf(data = bgd_borders, aes(color = "Bangladesh border", linetype = "Bangladesh border"), size = 4, fill = NA, color = "black") +
  scale_fill_manual(name = "", 
                    values = c("Targeted Points" = "red", 
                               "Respondents Browser Location" = "blue", 
                               "Respondents Qualtrics Location" = "skyblue"),
                    guide = guide_legend(order = 1, nrow = 1)) +
  scale_shape_manual(name = "", 
                     values = c("Targeted Points" = 21, 
                                "Respondents Browser Location" = 21, 
                                "Respondents Qualtrics Location" = 24),
                     guide = guide_legend(order = 1, nrow = 1)) +
  scale_linetype_manual(name = "", values = c("Bangladesh border" = "solid"),
                        guide = guide_legend(order = 2, nrow = 1)) +

  labs(
    title = "Bangladesh R1 Spatial Targeting"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom", 
    legend.box = "vertical", 
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.text = element_text(size = 9),
    legend.spacing.x = unit(0.25, 'cm'),
    legend.key.height = unit(0.1, "lines"),  # Adjust this for height of individual keys
    axis.text = element_text(color = "black") # Change axis labels to be black
  )
bgd_plt

ggsave("~/Documents/GitHub/Adaptation_Observatory/response_analysis/results/response_visuals/BGD/R1/clean_data_vis/bgd_R1_targeting_map.png", plot = bgd_plt, width = 10, height = 7, dpi = 300, bg = "white")
# ---- ind plot ----
ind_plt <- ggplot() +
  geom_sf(data = ind_sf_browser_intersect, aes(fill = "Respondents Browser Location", shape = "Respondents Browser Location"), color = "blue", size = 0.25, alpha = 0.5) + 
  geom_sf(data = ind_sf_intersect, aes(fill = "Respondents Qualtrics Location", shape = "Respondents Qualtrics Location"), color = "skyblue", size = 0.25, alpha = 0.2) + 
  geom_sf(data = ind_fb_sf_buffered, aes(fill = "Targeted Points", shape = "Targeted Points"), alpha = 1, color = NA) +
  geom_sf(data = ind_borders, aes(color = "India border", linetype = "India border"), size = 4, fill = NA, color = "black") +
  scale_fill_manual(name = "", 
                    values = c("Targeted Points" = "red", 
                               "Respondents Browser Location" = "blue", 
                               "Respondents Qualtrics Location" = "skyblue"),
                    guide = guide_legend(order = 1, nrow = 1)) +
  scale_shape_manual(name = "", 
                     values = c("Targeted Points" = 21, 
                                "Respondents Browser Location" = 21, 
                                "Respondents Qualtrics Location" = 24),
                     guide = guide_legend(order = 1, nrow = 1)) +
  scale_linetype_manual(name = "", values = c("India border" = "solid"),
                        guide = guide_legend(order = 2, nrow = 1)) +

  labs(
    title = "India R1 Spatial Targeting"
  ) +
  theme_minimal() +
  theme(
    legend.position = "bottom", 
    legend.box = "vertical", 
    plot.title = element_text(face = "bold", hjust = 0.5),
    legend.text = element_text(size = 9),
    legend.spacing.x = unit(0.25, 'cm'),
    legend.key.height = unit(0.1, "lines"),  # Adjust this for height of individual keys
    axis.text = element_text(color = "black") # Change axis labels to be black
  )
ind_plt

ggsave("~/Documents/GitHub/Adaptation_Observatory/response_analysis/results/response_visuals/IND/R1/clean_data_vis/ind_R1_targeting_map.png", plot = ind_plt, width = 10, height = 7, dpi = 300, bg = "white")

```

# Geospatial Stat. Analysis 

 I am trying to determine the distance from browser location and fb ad location for each ad dropped, but I don't think it makes the most sense to collect the distance of all points vs all ads bc they were likely not close enough to interact. So we're going to aggregate the bgd_sf_browser_intersect points using the strata bgd_fb_sf_buffered. these are two sf objects.

```{r}
distances <- st_distance(bgd_sf_browser_intersect, bgd_fb_sf_buffered)
length(distances)
```


- Assigned unique stratum_id to all fb ads
- Using knn, assigned panelist browser location to appropriate ad stratum_id 
- Calculated the avg distance from ad location for each stratum_id
- Used these avg in histogram to see avg distance distribution across all ads
- Calculated the % of unique ad id's that were assigned to panelist browser , displayed as "Ad Success Rate"
- All distances were less than 0.5 km for BGD
- All distances were less than 2 km for IND 

The ad success rates:

BGD: 306/855 * 100 = 35.79 % of the respondents were within a 1 km distance

IND: 184/850 * 100 = 19.37 % of the respondents were within a 2 km distance


Follow up question I want to tackle over the weekend: on avg, how many responses per ad?

I want a break down of the number of responses for each stratum_id

```{r bgd_ad_strata_dist}
library(dplyr)
library(nngeo)

# Add identifiers back to the buffered Facebook ad data
bgd_fb_sf_buffered <- bgd_fb_sf_buffered %>%
  mutate(stratum_id = row_number())

bgd_fb_sf_buffered

# Assuming bgd_sf_browser_intersect and bgd_fb_sf_buffered are your sf objects

# Assign a unique stratum ID to each buffered ad zone
bgd_fb_sf_buffered <- bgd_fb_sf_buffered %>%
  mutate(stratum_id = row_number())

# ---- knn assignment of stratum_id for ads ----
# Find the nearest buffered Facebook ad zone for each browser location using KNN
nearest_ad <- st_nn(bgd_sf_browser_intersect, bgd_fb_sf_buffered)

length(unique(nearest_ad))

# Join the browser locations with the nearest ad zones
browser_with_ad <- bgd_sf_browser_intersect %>%
  mutate(stratum_id = nearest_ad) %>%
  # extract list contents
  unnest(cols = stratum_id)
  

head(browser_with_ad, 5)
colnames(browser_with_ad)
# how many unique ad ids are represented here?
# aka how successful were our 855 ads?
# okay 306, not great, but not 0.
length(unique(browser_with_ad$stratum_id))
# our sucess rate here was 35.8
ad_success_rate <- as.numeric(length(unique(browser_with_ad$stratum_id))) / as.numeric(length(bgd_fb_sf_buffered$stratum_id)) * 100


# Check if any browser locations did not get assigned a stratum ID
unassigned <- browser_with_ad[is.na(browser_with_ad$stratum_id), ]

# If there are unassigned browser locations, handle them accordingly
if (nrow(unassigned) > 0) {
  # You can choose to assign a default stratum ID or handle them in another way
  # For example:
  browser_with_ad <- bind_rows(browser_with_ad, mutate(unassigned, stratum_id = NA))
}

unassigned
# Now, every browser location should have a corresponding stratum ID

# ---- distance calc for each ad strata ---- 
# Calculate distance for each stratum_id group
distance_table <- browser_with_ad %>%
  group_by(stratum_id) %>%
  # count how many responses per ad?
  summarize(num_responses = n_distinct(response_id),
            # calculate Mean distance in kilometers for each ad strata
            distance_km = mean(sf::st_distance(geometry, bgd_fb_sf_buffered), na.rm = TRUE) / 1000) %>% 
  mutate(distance_km = units::set_units(distance_km, "km")) 

nrow(distance_table)

distance_table


# Calculate the maximum distance
max_distance_bgd <- max(distance_table$distance_km)


# Define breaks for binning
breaks <- c(0, 0.3, 0.5, 1)

# Create labels for the bins
labels <- c("0-0.3 km","0.3-0.5 km", "0.5-1 km")

# Cut the distances into bins
distance_table <- distance_table %>%
  mutate(distance_bin = cut(distance_km, breaks = breaks, labels = labels, include.lowest = TRUE))

# Create the histogram
bgd_hist <- ggplot(distance_table, aes(x = distance_bin)) +
  
  geom_bar(stat = "count", fill = "skyblue", color = "black") +
  
  labs( title = "BGD Mean Strata Distribution of Distances by Nearest Ad Location (km)",
        subtitle = "Adaptation Observatory: April 15, 2024 Responses",
        x = "Distance Range (km)",
        y = "Frequency Count") +
  
  theme_minimal() +
  
  theme(
    plot.title = element_text(size = 15,
                              hjust = 0.5),
    
    plot.subtitle = element_text(size = 12,
                                 hjust = 0.5),
    
    axis.text.x = element_text(hjust = 1)) +
  
  geom_rect(
    aes(xmin = 2,
        xmax = 3.5, 
        ymin = 75, 
        ymax = 125),
    fill = "grey90",
    color = "black",
    size = 0.5,
    inherit.aes = FALSE,
    alpha = 0.1
  ) +
  # Add annotation
  annotate(
    "text",
    x = 2.75, 
    y = 85, 
    label = paste0("Maximum distance is ", round(max_distance_bgd, 3), " (km)"),
    hjust = 0.5,
    vjust = -1,
    color = "black",
    size = 5
  ) +
  geom_curve(
    aes(x = 2, y = 100, xend = 2, yend = 10),
    curvature = 0.3,
    color = "black",
    arrow = arrow(length = unit(0.3, "cm"))
  ) +
  geom_text(
    x = 2.75, 
    y = 150, 
    label = paste0("Ad Success Rate: ", round(ad_success_rate, 2), "%"),
    hjust = 0.5,
    vjust = -1,
    color = "black",
    size = 6
  )

bgd_hist

ggsave(here("response_analysis/results/response_visuals/BGD/R1/clean_data_vis/bgd_R1_targeting_dist_hist.png"), plot = bgd_hist, width = 10, height = 7, dpi = 300, bg = "white")
```

```{r ind_ad_strata_dist}
library(dplyr)
library(nngeo)

# Add identifiers back to the buffered Facebook ad data
ind_fb_sf_buffered <- ind_fb_sf_buffered %>%
  mutate(stratum_id = row_number())

#ind_fb_sf_buffered

# Assuming bgd_sf_browser_intersect and bgd_fb_sf_buffered are your sf objects

# Assign a unique stratum ID to each buffered ad zone
ind_fb_sf_buffered <- ind_fb_sf_buffered %>%
  mutate(stratum_id = row_number())

# ---- knn assignment of stratum_id for ads ----
# Find the nearest buffered Facebook ad zone for each browser location using KNN
nearest_ad_ind <- st_nn(ind_sf_browser_intersect, ind_fb_sf_buffered)

#length(unique(nearest_ad))

# Join the browser locations with the nearest ad zones
browser_with_ad_ind <- ind_sf_browser_intersect %>%
  mutate(stratum_id = nearest_ad_ind) %>%
  # extract list contents
  unnest(cols = stratum_id)
  

head(browser_with_ad_ind, 5)



# how many unique ad ids are represented here?
# aka how successful were our 950 ads?
# okay 184, not great, but not 0.
length(unique(browser_with_ad_ind$stratum_id))
# our sucess rate here was 19.4
ad_success_rate_ind <- as.numeric(length(unique(browser_with_ad_ind$stratum_id))) / as.numeric(length(ind_fb_sf_buffered$stratum_id)) * 100

# Check if any browser locations did not get assigned a stratum ID
unassigned_ind <- browser_with_ad_ind[is.na(browser_with_ad_ind$stratum_id), ]

# If there are unassigned browser locations, handle them accordingly
if (nrow(unassigned) > 0) {
  # You can choose to assign a default stratum ID or handle them in another way
  # For example:
  browser_with_ad_ind <- bind_rows(browser_with_ad_ind, mutate(unassigned, stratum_id = NA))
}

unassigned_ind
# Now, every browser location should have a corresponding stratum ID

# ---- distance calc for each ad strata ---- 
# Calculate distance for each stratum_id group
distance_table_ind <- browser_with_ad_ind %>%
  group_by(stratum_id) %>%
  # count how many responses per ad?
  summarize(num_responses = n_distinct(response_id),
            # Mean distance in kilometers for each ad strata
            distance_km = mean(sf::st_distance(geometry, ind_fb_sf_buffered), na.rm = TRUE) / 1000) %>% 
  mutate(distance_km = units::set_units(distance_km, "km")) 



nrow(distance_table_ind)

distance_table_ind


# Calculate the maximum distance
max_distance <- max(distance_table_ind$distance_km)


# Define breaks for binning
breaks <- c(0, 0.3, 0.5, 1, 1.5, 2)

# Create labels for the bins
labels <- c("0-0.3 km","0.3-0.5 km", "0.5-1.0 km", "1.0-1.5 km", "1.5-2 km")

# Cut the distances into bins
distance_table_ind <- distance_table_ind %>%
  mutate(distance_bin = cut(distance_km, breaks = breaks, labels = labels, include.lowest = TRUE))

# Create the histogram
ind_hist <- ggplot(distance_table_ind, aes(x = distance_bin)) +
  
  geom_bar(stat = "count", fill = "skyblue", color = "black") +
  
  labs( title = "IND Mean Strata Distribution of Distances by Nearest Ad (km)",
        subtitle = "Adaptation Observatory: April 15, 2024 Responses",
        x = "Distance Range (km)",
        y = "Frequency Count") +
  
  theme_minimal() +
  
  theme(
    plot.title = element_text(size = 15,
                              hjust = 0.5),
    
    plot.subtitle = element_text(size = 12,
                                 hjust = 0.5),
    
    axis.text.x = element_text(hjust = 1)) +
  
  geom_rect(
    aes(xmin = 3,
        xmax = 4.9, 
        ymin = 45, 
        ymax = 25),
    fill = "grey90",
    color = "black",
    size = 0.5,
    inherit.aes = FALSE,
    alpha = 0.1
  ) +
  # Add annotation
  annotate(
    "text",
    x = 3.95, 
    y = 29, 
    label = paste0("Maximum distance is ", round(max_distance, 2), " (km)"),
    hjust = 0.5,
    vjust = -1,
    color = "black",
    size = 5
  ) +
  geom_curve(
    aes(x = 3, y = 35, xend = 2.8, yend = 10),
    curvature = 0.3,
    color = "black",
    arrow = arrow(length = unit(0.3, "cm"))
  ) +
  geom_text(
    x = 3.95, 
    y = 55, 
    label = paste0("Ad Success Rate: ", round(ad_success_rate_ind, 2), "%"),
    hjust = 0.5,
    vjust = -1,
    color = "black",
    size = 6
  )

ind_hist

ggsave(here("response_analysis/results/response_visuals/IND/R1/clean_data_vis/ind_R1_targeting_dist_hist.png"), plot = ind_hist, width = 10, height = 7, dpi = 300, bg = "white")
```

```{r tot}
# Create a table with total distances averaged
# If you want total distances averaged, it's a bit different, here's how you can do it:
total_distance <- sum(distance_table$distance_km)
total_count <- nrow(distance_table)
average_distance <- total_distance / total_count

average_distance_table <- data.frame(total_distance = total_distance, total_count = total_count, average_distance = average_distance)
```

This workflow does include ads that did not have respondents associated to it. The length of this output is 855

```{r}
library(units)

# Initialize an empty data frame to store the results
distance_table2 <- data.frame(stratum_id = numeric(), distance_km = numeric())

# Loop through each unique stratum_id
for (stratum_id in unique(bgd_fb_sf_buffered$stratum_id)) {
  # Subset browser_with_ad for the current stratum_id
  subset_browser <- browser_with_ad[browser_with_ad$stratum_id == stratum_id, ]
  # Subset bgd_fb_sf_buffered for the current stratum_id
  subset_buffered <- bgd_fb_sf_buffered[bgd_fb_sf_buffered$stratum_id == stratum_id, ]
  # Calculate distances between points in subset_browser and subset_buffered
  distances2 <- st_distance(subset_browser$geometry, subset_buffered$geometry) / 1000  # Convert to kilometers
  # Calculate the mean distance for this stratum_id
  mean_distance <- mean(distances2, na.rm = TRUE)
  # Append the result to the distance_table
  distance_table2 <- rbind(distance_table2, data.frame(stratum_id = stratum_id, distance_km = mean_distance))
}

# Set units for distance_km
distance_table2$distance_km <- units::set_units(distance_table2$distance_km, "km")


distance_table2
# Print distance_table
print(distance_table2)
```









I want to create an object that only contains the points that overlap respondents browser location and targeted points

I then want that object to compare the number of those observations we collected in the expected regions
I want to compare the amount of success vs failure 


Leveraging the workflow Emma outlined above, I isolated regions where respondents browser location intersected with a 1km buffer around the fb ads that had been deployed.

Comparing the count of respondents within those areas to the count of fb ads that were deployed:
- BGD success rate was 41.52%
- IND success rate was 6%

One reasoning behind this discrepancy can be explained by the data limitations caused by the IND flood data. The satellite data provided by Microsoft Planetary Computer for IND was in an angled position. Regardless of the projection, it was extremely difficult getting the boundaries to align. When generating geopoints for IND, there was less confidence in the accuracy of the points being within their expected ADM1 and ADM2 boundaries. 

We deployed a large amount of ads in the NE region of IND, near BGD. Those ads did not receive a lot of feedback response. I'm curious if the ads were competing with the BGD campaign because we received a good amount of BGD income responses that were in Rupees, indicating they may be on the border / do business in IND.

Currently working on comparing the regions at risk of flood exposure to the respondents browser location that intersected, with no buffers applied. This will confirm with the flood data that despite the disparity in our ad success for IND, the majority of respondents for IND are located on the coast and in regions at risk to flooding.

The intersection process is taking a really long time to complete and I haven't been able to get it to work properly yet. Once that is running right, I will provide the sucess rates for targetting regions at risk of flood exposure. 

-- Other Updates --
Successfully translated the responses, in the process of deep cleaning the responses. I expect to have that complete by the end of this week and will use that data to generate stacked plots. I will upload those here upon completion. 


-- Next time 
Confirming with the flood data, I'm really happy to see that despite the disparity in our ad success for IND, the majority of respondents for IND are located on the coast and in regions at risk to flooding.
- BGD success rate was
- IND success rate was

```{r ad_response_overlap}
# BGD 
# Assuming you have two spatial datasets: respondents' browser locations and targeted points
# Replace 'respondents' and 'targeted' with your actual datasets

# respondent locations inside of country boundaries
obs_in_expected_regions_bgd <- st_intersection(bgd_sf_browser_intersect, bgd_fb_sf_buffered)

# expected points 
bgd_expected_regions <- bgd_fb_sf_buffered

bgd_expected_regions


# Step 2: Count observations in expected regions
success_count <- nrow(obs_in_expected_regions_bgd)
failure_count <- nrow(bgd_expected_regions) - success_count

# Step 3: Compare success vs. failure
# Print or use the counts as needed
print(success_count)
print(failure_count)

rate_of_success_bgd <- (success_count / nrow(bgd_expected_regions)) * 100

rate_of_success_bgd






# IND 
# Assuming you have two spatial datasets: respondents' browser locations and targeted points
# Replace 'respondents' and 'targeted' with your actual datasets

# respondent locations inside of country boundaries
obs_in_expected_regions_ind <- st_intersection(ind_sf_browser_intersect, ind_fb_sf_buffered)

# expected points 
ind_expected_regions <- ind_fb_sf_buffered

#ind_expected_regions


# Step 2: Count observations in expected regions
success_count_ind <- nrow(obs_in_expected_regions_ind)
failure_count <- nrow(ind_expected_regions) - success_count_ind

# Step 3: Compare success vs. failure
# Print or use the counts as needed
print(success_count_ind)
print(failure_count)

rate_of_success_ind <- (success_count_ind / nrow(ind_expected_regions)) * 100

rate_of_success_ind
```

```{r bgd_flood_exposure_response_rate}
bgd_floods <-read_stars(here('../data/country_datas/bgd_flooding.tif')) 

# Transform the flood data into a SpatRaster object
bgd_floods_rast <- rast(bgd_floods)


# creating a mask to filter for areas at risk of experiencing flooding in 75th percentile
bgd_flood_rast <- bgd_floods_rast >= 86 

# subset 1km resolution
layer4 <- terra::subset(bgd_flood_rast, 4)


# create sf object for intersection comparision
bgd_floods_1km <- layer4 %>% 
  st_as_stars %>%
  st_as_sf() %>% 
  st_make_valid()

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----  Set of Checks & CRS Corrections  ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# boolean test to see if resolutions match
#resolution(bgd_floods_1km) == resolution(bgd_sf_browser_intersect)

st_crs(bgd_floods_1km) <- st_crs(bgd_sf_browser_intersect)

#st_crs(bgd_floods_1km) == st_crs(bgd_sf_browser_intersect)


# Subset bgd_floods_1km where band4 == TRUE
# these are regions with flood exposure 
subset_floods <- bgd_floods_1km[bgd_floods_1km$`band4` == TRUE, ] %>% 
  st_make_valid()
```

Joins are one of the most computationally taxing tools. When working with joins, it is always advised to aggregrate your data as much as possible prior to performing the join. This will simplify and reduce the amount of correlations the computer will have to sort though when pairing. If we tried to perform this spatial join without any aggregation prior, it would take a significantly long period of time 

ideas:
- unionize the true flood data
- rasterize the true flood data
- NOT useful was simplifying 

```{r unionize}
floods_unionized <- st_union(subset_floods) %>% 
  st_make_valid()
```

```{r}
st_crs(floods_unionized) <- st_crs(bgd_sf_browser_intersect)

plot(floods_unionized)
plot(bgd_sf_browser_intersect)
```

```{r bgd_flood_intersection}
# Simplify geometries
#floods_simplified <- st_simplify(subset_floods) %>% 
 # st_make_valid()

#obs_in_flood_regions_bgd <- st_intersection(bgd_sf_browser_intersect, floods_simplified)

#bgd_sf_browser_intersect <- bgd_sf_browser_intersect %>% 
 # st_make_valid()

# Perform intersection to find points in flood prone regions
obs_in_flood_regions_bgd <- st_intersection(bgd_sf_browser_intersect, floods_unionized)

obs_in_flood_regions_bgd <- st_intersection(floods_unionized, bgd_sf_browser_intersect)

# ---- Calling cores and doing in parallel ----
#library(foreach)
#library(doParallel)

# Set the number of cores to use
#num_cores <- detectCores()

# Initialize parallel processing
#cl <- makeCluster(num_cores)
#registerDoParallel(cl)


# Perform intersection in parallel
#obs_in_flood_regions_bgd <- foreach(i = 1:num_cores, .packages = c("sf")) %dopar% {
 # st_intersection(bgd_sf_browser_intersect, subset_floods)
#}

# Combine the results from parallel execution
#obs_in_flood_regions_bgd <- do.call(rbind, obs_in_flood_regions_bgd)

# Stop parallel processing
#stopCluster(cl)

# ---- Counting observations ----
# Step 2: Count observations in expected regions
success_flood_count_bgd <- nrow(obs_in_flood_regions_bgd)
failure_flood_count_bg <- nrow(subset_floods) - success_count_bgd

# Step 3: Compare success vs. failure
# Print or use the counts as needed
print(success_flood_count_bgd)
print(failure_flood+count_bgd)

rate_of_success_floods_bgd <- (success_flood_count_bgd / nrow(subset_floods)) * 100

rate_of_success_floods_bgd
```


```{r ind_flood_exposure_response_rate}
ind_floods <-read_stars(here('../data/country_datas/bgd_flooding5.tif')) 


# Transform the flood data into a SpatRaster object
ind_floods_rast <- rast(ind_floods)


# creating a mask to filter for areas at risk of experiencing flooding in 75th percentile
ind_flood_rast <- ind_floods_rast >= 86 

# subset 1km resolution
layer4 <- terra::subset(ind_flood_rast, 4)

ind_floods_sf<- ind_floods  %>% 
  st_as_stars() %>% 
  st_as_sf() %>% 
  st_make_valid() 


# select 1km resolution from tiff:vector data
ind_flood_1km_sf <- ind_floods_sf %>% dplyr::select(ind_flooding5.tif.V4) 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----  Set of Checks & CRS Corrections  ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# boolean test to see if resolutions match
resolution(ind_floods_1km_sf) == resolution(ind_sf_browser_intersect)

st_crs(ind_floods_1km_sf) <- st_crs(ind_sf_browser_intersect)

st_crs(ind_floods_1km_sf) == st_crs(ind_sf_browser_intersect)


# Subset bgd_floods_1km where band4 == TRUE
# these are regions with flood exposure 
subset_floods <- ind_floods_1km[ind_flood_1km$`band4` == TRUE, ] %>% 
  st_make_valid()


# Perform intersection to find points in flood prone regions
obs_in_flood_regions_ind <- st_intersection(ind_sf_browser_intersect, subset_floods)


# Step 2: Count observations in expected regions
success_count_ind <- nrow(obs_in_flood_regions_ind)
failure_count <- nrow(subset_floods) - success_count_ind

# Step 3: Compare success vs. failure
# Print or use the counts as needed
print(success_flood_count_ind)
print(failure_flood+count_ind)

rate_of_success_floods_ind <- (success_flood_count_ind / nrow(subset_floods)) * 100

rate_of_success_floods_ind

```



ALSO 

Working on taking each respondents location and calculating the distance to the nearest sample point with histogram by dist. bins.

```{r}
```


```{r}
```



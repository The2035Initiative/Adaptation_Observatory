---
author: "Sofia Ingersoll"
title: "Adaptation Observatory: Survey Response Demographics (Bangladesh & India)"
date: "2024-04-14"
output:
  html_document:
    code_folding: show
    toc: true
    toc_float: true
---

## Set Up
```{r message = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

#..........................load packages.........................
library(tidyverse)        # load the tidyverse package to assist with data wrangling & cleaning 

library(patchwork)        # load the patchwork package to assist in plot composition (displaying multiple data visualizations) 

library(showtext)         # load the showtext to more easily use fonts
library(here)

library(leaflet)
library(osmextract) 
library(units)
library(sf)
library(sfheaders)
library(sp)
library(raster)
library(tmap)
library(terra)
library(stars)
library(ggtext)
library(units)

library(tidyverse)
library(treemap)
library(sunburstR)
library(purrr)
```

## Load & Wrangle Survey Demographic Data
```{r warning = FALSE}
bgd_file <- file.path(here("..","data","adapt_observ_survey_responses", "Observatory_Bangladesh_2024_04_15.csv"))
  
ind_file <- file.path(here("..","data","adapt_observ_survey_responses", "Observatory_India_2024_04_15.csv"))

bgd_responses <- read_csv(bgd_file, show_col_types = FALSE)
  
ind_responses <-read_csv(ind_file, show_col_types = FALSE)
```

### Filtering only people with cell phone consent and location (gps permission, location_lat/long)

Language: how many dialects



#### Make a csv of the questions that were sent out for the first round
```{r}
survey_questions <- bgd_responses[1, ]

survey_questions

write.csv(survey_questions, file = "adaptation_observatory_survey_questions_round_1.csv")
```

## BGD
```{r}
colnames(bgd_responses)

unique(bgd_responses$consent_cellphone)
unique(bgd_responses$consent_agreed)
unique(bgd_responses$consent_age)

# Let's only look at potential panel candidates 
# bgd reduce to 2262 people
bgd_data <- bgd_responses %>%
  janitor::clean_names() %>% 
  filter(consent_age == "Yes") %>% 
  filter(consent_agreed == "I agree to participate") %>% 
  filter(consent_cellphone == "Yes") %>%
  filter(gps_permission == "Yes") %>% 
  filter(!is.na(comp_phone1_try1)) %>% 
  filter(!is.na(comp_phone1_try2)) %>% 
  # Count the occurrences of each IPAddress
  group_by(ip_address) %>%
  mutate(no_surverys_submitted = n()) %>%
  # Correct dtype of lat/long
  mutate(location_longitude = as.numeric(location_longitude),
         location_latitude = as.numeric(location_latitude)) %>%
  # remove incomplete geolocations
  filter(complete.cases(location_latitude, location_longitude)) %>%
  ungroup() 
  
bgd_data

# this confirms that all of the respondants here have unique 
length(unique(bgd_data$response_id))

# let's further investigate the variance in responses
dup_ip_bgd <- bgd_data %>% 
  filter(no_surverys_submitted > 1) %>% 
  mutate(income_brackets = case_when(
    demo_income <= 300000 ~ "0% Tax Rate (Up to BDT 300,000)",
    demo_income > 300000 & demo_income <= 400000 ~ "5% Tax Rate (BDT 300,001 - 400,000)",
    demo_income > 400000 & demo_income <= 700000 ~ "10% Tax Rate (BDT 400,001 - 700,000)",
    demo_income > 700000 & demo_income <= 1100000 ~ "15% Tax Rate (BDT 700,001 - 1,100,000)",
    demo_income > 1100000 & demo_income <= 1600000 ~ "20% Tax Rate (BDT 1,100,001 - 1,600,000)",
    demo_income > 1600000 ~ "25% Tax Rate (Above BDT 1,600,000)") 
    )


length(unique(dup_ip_bgd$comp_phone1_try1))


write.csv(dup_ip_bgd, file = "../clean_data/adaptation_observatory_bgd_cleaned_duplicate_survey_questions_R1.csv")
```

####  Make a csv of all the responser ids
```{r}
bgd_respondents <- bgd_data[,"response_id"]

# Create a folder named "survey_response_ids" if it doesn't exist
dir.create("survey_response_ids", showWarnings = FALSE)

# Set the file path including the folder
file_path <- file.path("survey_response_ids", "bgd_survey_response_ids.csv")

# save it as a csv
write_csv(bgd_respondents, file = file_path)
```


# What tax brackets are represented here? `demo_income`
[Policy for personal income tax in Bangladesh](https://bida.gov.bd/details/what-policy-personal-income-tax-bangladesh) was determined using the information provided by the Bangladesh Investment Development Authority (BIDA) Prime Ministerâ€™s Office.
```{r}
# Define the income thresholds 
bgd_data <- bgd_data %>% 
  mutate(income_brackets = case_when(
    demo_income <= 300000 ~ "0% Tax Rate (Up to BDT 300,000)",
    demo_income > 300000 & demo_income <= 400000 ~ "5% Tax Rate (BDT 300,001 - 400,000)",
    demo_income > 400000 & demo_income <= 700000 ~ "10% Tax Rate (BDT 400,001 - 700,000)",
    demo_income > 700000 & demo_income <= 1100000 ~ "15% Tax Rate (BDT 700,001 - 1,100,000)",
    demo_income > 1100000 & demo_income <= 1600000 ~ "20% Tax Rate (BDT 1,100,001 - 1,600,000)",
    demo_income > 1600000 ~ "25% Tax Rate (Above BDT 1,600,000)") 
    )



write.csv(bgd_data, file = "../clean_data/adaptation_observatory_bgd_cleaned_survey_questions_R1.csv")
```



# Data Maid BGD


histogram of basic distribution of demographics
people who give location vs done: how are they different
how are the location and ip_address & phone numbers same or different
do this for duplicate subset


curious about 
distribution of languages people took the survey in (11 regional languages, qualtrix recorded the type as a metric)

We expect people falling within validated bufffers to report seeing a flood more often or experience a negative experience more often than those outside of our AOI.

Considerations moving forward
- who's inside and outside AOI || DROP 
- who among the respondents are most likely to contribute to the panel moving forward


#### Comparing differences between geo_lat, geo_lon vs location_lat, location_lon


```{r}
suppressPackageStartupMessages(library(dataMaid))

bgd <- bgd_data %>% 
  # remove first two rows
  slice(-c(1, 2)) 

makeDataReport(bgd,
               output = "html",
               replace = TRUE,
               reportTitle = "Bangladesh Adaptation Observation Survey Responses, Retrieval Date 15 April 2024.")

makeDataReport(dup_ip_bgd,
               output = "html",
               replace = TRUE,
               reportTitle = "Bangladesh Adaptation Observation Duplicate IP Address Survey Responses, Retrieval Date 15 April 2024.")
```

## IND
```{r}
# Let's only look at potential panel candidates 
# ind reduce to 1229 people
ind_data <- ind_responses %>%
  janitor::clean_names() %>% 
  filter(consent_age == "Yes") %>% 
  filter(consent_agreed == "I agree to participate") %>% 
  filter(consent_cellphone == "Yes") %>%
  filter(gps_permission == "Yes") %>% 
  filter(!is.na(comp_phone1_try1)) %>% 
  filter(!is.na(comp_phone1_try2)) %>% 
  # Count the occurrences of each IPAddress
  group_by(ip_address) %>%
  mutate(no_surverys_submitted = n()) %>%
  # Correct dtype of lat/long
  mutate(location_longitude = as.numeric(location_longitude),
         location_latitude = as.numeric(location_latitude)) %>%
  ungroup() 
  
ind_data

# this confirms that all of the respondants here have unique 
length(unique(ind_data$response_id))

# let's further investigate the variance in responses
dup_ip_ind <- ind_data %>% 
  filter(no_surverys_submitted > 1) %>% 
  mutate(income_brackets = case_when(
    demo_income <= 300000 ~ "0% Tax Rate (Up to BDT 300,000)",
    demo_income > 300000 & demo_income <= 400000 ~ "5% Tax Rate (BDT 300,001 - 400,000)",
    demo_income > 400000 & demo_income <= 700000 ~ "10% Tax Rate (BDT 400,001 - 700,000)",
    demo_income > 700000 & demo_income <= 1100000 ~ "15% Tax Rate (BDT 700,001 - 1,100,000)",
    demo_income > 1100000 & demo_income <= 1600000 ~ "20% Tax Rate (BDT 1,100,001 - 1,600,000)",
    demo_income > 1600000 ~ "25% Tax Rate (Above BDT 1,600,000)") 
    )

length(unique(dup_ip_ind$comp_phone1_try1))


write.csv(dup_ip_ind, file = "adaptation_observatory_ind_cleaned_duplicate_survey_questions_R1.csv")
```

# What tax brackets are represented here? `demo_income`
[Latest Income Tax Slab and Rates - FY 2023-24 | AY 2024-25](https://economictimes.indiatimes.com/wealth/income-tax-slabs?from=mdr) was determined using the information provided by the India Economic Times.
```{r}
# Define the income thresholds 
ind_data <- ind_data %>% 
  mutate(income_brackets = case_when(
    demo_income <= 300000 ~ "0% Tax Rate (Up to Rs. 3,00,000)",
    demo_income > 300000 & demo_income <= 600000 ~ "5% Tax Rate (Rs. 3,00,001 - 6,00,000)",
    demo_income > 600000 & demo_income <= 900000 ~ "10% Tax Rate (Rs. 6,00,001 - 9,00,000)",
    demo_income > 900000 & demo_income <= 1200000 ~ "15% Tax Rate (Rs. 9,00,001 - 12,00,000)",
    demo_income > 1200000 & demo_income <= 1500000 ~ "20% Tax Rate (Rs. 12,00,001 - 15,00,000)",
    demo_income > 1500000 ~ "30% Tax Rate (Above Rs. 15,00,000)") 
    )


write.csv(ind_data, file = "adaptation_observatory_ind_cleaned_survey_questions_R1.csv")
```

```{r}
ind <-ind_data %>% 
  # remove first two rows
  slice(-c(1, 2)) 

makeDataReport(ind,
               output = "html",
               replace = TRUE,
               reportTitle = "India Adaptation Observation Survey Responses, Retrieval Date 15 April 2024.")

makeDataReport(dup_ip_ind,
               output = "html",
               replace = TRUE,
               reportTitle = "India Adaptation Observation Duplicate IP Address Survey Responses, Retrieval Date 15 April 2024.")
```

####  Make a csv of all the responser ids
```{r}
ind_respondents <- ind_data[,"response_id"]

# Create a folder named "survey_response_ids" if it doesn't exist
dir.create("survey_response_ids", showWarnings = FALSE)

# Set the file path including the folder
file_path <- file.path("survey_response_ids", "ind_survey_response_ids.csv")

# save it as a csv
write_csv(ind_respondents, file = file_path)
```

# How many of these participants are within the shape file we originally used to generate geo-points?

validate & quanitfy % accuracy
from that sample: assess group representation benchmarks
(gender, income, age) start with this for now

Explicit test of wht fraction of locations are located within the spatial polygon we were sampling

within 5-10km buffer potentially as a result of cell-towers

How affective have we been finding indiividuals we hoped to find in these areas


distribution of languages people took the survey in (11 regional languages, qualtric recorded the type as a metric)

We expect people falling within validated bufffers to report seeing a flood more often or experience a negative experience more often than those outside of our AOI.

Considerations moving forward
- who's inside and outside AOI || DROP 
- who among the respondents are most likely to contribute to the panel moving forward


```{r bgd_sf_object}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----          Load Pop. Den Data       ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# reading in the data
bgd_pop_den <- read_stars(here("../data/country_datas/bgd_pd_2020_1km.tif"))


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----        SpatRaster the Data        ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# convert to formal class SpatRaster
bgd_pop_den_rast <- rast(bgd_pop_den)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----      Determine 95th Percentile    ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
raster_values <- values(bgd_pop_den_rast)
q95 <- quantile(raster_values, probs = 0.95,
                na.rm = TRUE)

bgd_hpd_rast <- bgd_pop_den_rast >= q95

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----        Load Flood Stacked Data.   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# load data
#bgd_floods <- read_stars(here("../data/country_datas/ind_flooding.tif"))

bgd_floods <-read_stars(here('../data/country_datas/ind_flooding5.tif')) 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----        SpatRaster the Data        ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Transform the flood data into a SpatRaster object
bgd_floods_rast <- rast(bgd_floods)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----   Filter for Flood Prone Regions  ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# creating a mask to filter for areas at risk of experiencing flooding in 75th percentile

q75 <- quantile(bgd_floods_rast, probs = 0.75,
                na.rm = TRUE)
bgd_flood_rast <- bgd_floods_rast >= q75 

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----  Set of Checks & CRS Corrections  ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# boolean test to see if resolutions match
ext(bgd_flood_rast) <- ext(bgd_hpd_rast)
crs(bgd_flood_rast) <- crs(bgd_hpd_rast)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----     Visualize 1km Res Flood Map   ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
layer4 <- terra::subset(bgd_flood_rast, 4)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----       Function to Overlay         ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# creating a function to overlay rasters of uneven dimensions
fun = function(x, y) {
  x*y
}


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----          Resample  w/ KNN         ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Resample the depth data, use method = "near" to use the nearest neighbor approach
floods_resample <- resample(layer4, y = bgd_hpd_rast, method = "near")

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----        Set of Optional Checks     ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Check that the depth data was resampled by stacking the rasters
pd_floods <- c(bgd_hpd_rast, floods_resample)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----    Visualize Regions at Risk      ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Use Lapp to create an overlay of the reclassified data
aoi <- lapp(c(bgd_hpd_rast, floods_resample), fun = fun) 


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----         isolate ror as sf obj     ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# create a df containing only geo-points for values = 1 or TRUE for hpd & flooding risk
# convert SpatRaster to sf object, 
ror <- aoi %>%
  st_as_stars %>% 
  st_as_sf() %>% 
  st_make_valid() 

# filter aoi spatraster to only contain values = 1. This reduced our no. observations to 12636.
ror <- ror %>% 
  filter(lyr1 == 1)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----  FAO boundaries to create strata  ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# reading in the data
bgd_adm1 <- st_read(here("../data/country_datas/geoBoundaries-IND-ADM1_simplified.geojson")) %>% 
  janitor::clean_names() %>% 
  mutate(adm1_boundary = shape_name)


bgd_adm2 <- st_read(here("../data/country_datas/geoBoundaries-IND-ADM2_simplified.geojson")) %>% 
  janitor::clean_names()%>% 
  mutate(adm2_boundary = shape_name)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#----    include providence boundaries  ----
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# join addresses and buffered points
bgd_aoi <- st_intersection(ror, bgd_adm1) %>% 
  dplyr::select(adm1_boundary, geometry) %>% 
  group_by(adm1_boundary) 

bgd_aoi <-  st_intersection(bgd_aoi, bgd_adm2) %>% 
  
  dplyr::select(adm1_boundary, adm2_boundary, geometry) %>% 
  
  group_by(adm1_boundary) %>% 
  group_by(adm2_boundary)
```

This join isn't perfect yet, it doesn't preserve the data I want to add to the plot. I need to give this more time.

```{r bgd_gis_check}
bgd_demo_sf <- bgd %>% 
  
  st_as_sf(coords = c("location_longitude", "location_latitude"), crs = 4326)


bgd_aoi <- st_join(bgd_aoi,
                   bgd_demo_sf,
                   #.pred = st_within
                   )
bgd_aoi

tmap_mode('view')

bgd_response_map <- tm_shape(bgd_aoi) +
  tm_basemap(leaflet::providers$Esri.WorldImagery) +
  tm_polygons(adm1_boundary,
              palette = 'Purples',
              fill = "grey80",
              alpha = 2) +
  tm_bubbles(alpha = 0.2,
             fill = 'ip_address',
             col = 'skyblue',) +
  tm_scalebar(position = c('left', 'bottom')) +
  tm_title("Adaptation Observatory Responses, Bangladesh (2024-4-15)")


bgd_response_map
```

**Color mapping // Opinion Mapping**
